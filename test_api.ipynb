{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932296d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import uvicorn\n",
    "import json\n",
    "import datetime\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "347c3288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8ad7f7502c4997b0a8fd4c9d092b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = './qwen/Qwen1.5-7B-Chat'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ee26013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n你好<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"你好\"}\n",
    "    ]\n",
    "\n",
    "# 调用模型进行对话生成\n",
    "input_ids = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ec301bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
       "         151645,    198, 151644,    872,    198, 108386, 151645,    198, 151644,\n",
       "          77091,    198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer([input_ids], return_tensors=\"pt\").to('cuda')\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08efef94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
       "         151645,    198, 151644,    872,    198, 108386, 151645,    198, 151644,\n",
       "          77091,    198]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45fe1289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
       "         151645,    198, 151644,    872,    198, 108386, 151645,    198, 151644,\n",
       "          77091,    198, 108386,   6313, 112169, 106184,  99553, 100364,   1773,\n",
       "         104139,  86119, 100631,  85106, 100703,   9370, 101037,  11319, 151645]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model.generate(model_inputs.input_ids,max_new_tokens=512)\n",
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0e37d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([108386,   6313, 112169, 106184,  99553, 100364,   1773, 104139,  86119,\n",
       "         100631,  85106, 100703,   9370, 101037,  11319, 151645],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "generated_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-llm-qwen1.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
